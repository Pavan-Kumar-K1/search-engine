{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b595c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the code below and write your observation in the next cell\n",
    "\n",
    "conn = sqlite3.connect(r\"C:\\Users\\k.udayasagar\\Downloads\\eng_subtitles_database.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8322c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"PRAGMA table_info('zipfiles')\")\n",
    "cols = cursor.fetchall()\n",
    "for col in cols:\n",
    "    print(col[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecce94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT * FROM zipfiles LIMIT 25000\"\"\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "\n",
    "count = 0\n",
    "\n",
    "def decode_method(binary_data):\n",
    "    global count\n",
    "    # Decompress the binary data using the zipfile module\n",
    "    # print(count, end=\" \")\n",
    "    count += 1\n",
    "    with io.BytesIO(binary_data) as f:\n",
    "        with zipfile.ZipFile(f, 'r') as zip_file:\n",
    "            # Assuming there's only one file in the ZIP archive\n",
    "            subtitle_content = zip_file.read(zip_file.namelist()[0])\n",
    "    \n",
    "    # Now 'subtitle_content' should contain the extracted subtitle content\n",
    "    return subtitle_content.decode('latin-1')  # Assuming the content is UTF-8 encoded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04247b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file_content'] = df['content'].apply(decode_method)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['name','file_content']]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b37e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33170f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(text, n,zx):\n",
    "    text = text.lower()\n",
    "    # Define regex pattern to match serial numbers and timestamps\n",
    "    pattern = r'\\d+\\s*?\\r?\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}\\s*?\\r?\\n'\n",
    "    \n",
    "    # Replace matched pattern with an empty string to remove it\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # Find the index of the first empty line\n",
    "    empty_line_index = cleaned_text.find('\\n\\n') + 2\n",
    "    \n",
    "    # Initialize count of removed non-empty lines\n",
    "    removed_lines = 0\n",
    "    \n",
    "    # Find the indices of the first n non-empty lines after the empty line\n",
    "    nonempty_line_indices = [m.start() for m in re.finditer(r'\\n', cleaned_text[empty_line_index:]) if not cleaned_text[empty_line_index:][m.start()].isspace()]\n",
    "    \n",
    "    # Remove the first n non-empty lines after the empty line\n",
    "    for index in nonempty_line_indices:\n",
    "        cleaned_text = cleaned_text[:empty_line_index + index] + re.sub(r'^.*?(\\r?\\n)', '', cleaned_text[empty_line_index + index:], count=1)\n",
    "        removed_lines += 1\n",
    "        if removed_lines == n:\n",
    "            break\n",
    "    cleaned_text = re.sub(r'<[^>]*>', '', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99416d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_content'] = df['file_content'].apply(preprocessing_data,args=(2,\"abc\"))\n",
    "\n",
    "print(df[['name', 'cleaned_content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_overlapping_chunks(text, window_size, overlap_size):\n",
    "    words = text.split()  # Split text into words\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + window_size\n",
    "        if end > len(words):\n",
    "            end = len(words)\n",
    "        chunk = ' '.join(words[start:end])  # Join words to form chunk\n",
    "        chunks.append(chunk)\n",
    "        start += window_size - overlap_size\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "# Assuming df is your DataFrame with 'cleaned_content' and 'name' columns\n",
    "\n",
    "# Define window size and overlap size\n",
    "window_size = 500  # 500 words\n",
    "overlap_size = 50  # Adjust this as needed\n",
    "\n",
    "# Create a new DataFrame to store the chunks\n",
    "chunked_df = pd.DataFrame(columns=['chunk', 'name'])\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for text, name in zip(df['cleaned_content'], df['name']):\n",
    "    # Generate overlapping chunks for the current text\n",
    "    chunks = generate_overlapping_chunks(text, window_size, overlap_size)\n",
    "    # Append the chunks and their original row index\n",
    "    for chunk in chunks:\n",
    "        chunked_df = chunked_df.append({'chunk': chunk, 'name': name}, ignore_index=True)\n",
    "\n",
    "# Display the chunked DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a57f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21de6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_df['bert_vector'] = chunked_df['chunk'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "import uuid  # Import the uuid module to generate unique IDs\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.create_collection(name=\"embedding_data\")\n",
    "\n",
    "for index, row in chunked_df.iterrows():\n",
    "    # Extract information from the row\n",
    "    name = row['name']\n",
    "    bert_vector = row['bert_vector']\n",
    "#     print(name)\n",
    "#     print(bert_vector)\n",
    "    \n",
    "    # Convert the bert_vector from NumPy ndarray to list\n",
    "    bert_vector_list = bert_vector.tolist()\n",
    "    \n",
    "    # Generate a unique ID for the chunk\n",
    "    chunk_id = str(uuid.uuid4())  # Generate a UUID as the ID\n",
    "    \n",
    "    # Add the chunk to ChromaDB\n",
    "    collection.add(ids=[chunk_id], embeddings=[bert_vector_list], metadatas=[{\"name\": name}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7ff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
